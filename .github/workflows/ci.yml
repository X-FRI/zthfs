name: CI

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

env:
  CARGO_TERM_COLOR: always

jobs:
  test:
    name: Test Suite
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Install Rust
      uses: dtolnay/rust-toolchain@stable
      with:
        components: rustfmt, clippy

    - name: Cache Cargo registry
      uses: actions/cache@v3
      with:
        path: |
          ~/.cargo/registry
          ~/.cargo/git
          target/
        key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}

    - name: Check formatting
      run: cargo fmt --all -- --check

    - name: Run clippy
      run: cargo clippy --all-targets --all-features -- -D warnings

    - name: Run tests
      run: cargo test --verbose

    - name: Generate test coverage
      run: |
        cargo install cargo-llvm-cov
        cargo llvm-cov --all-features --workspace --lcov --output-path lcov.info

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: lcov.info
        fail_ci_if_error: false

  security:
    name: Security Audit
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Install Rust
      uses: dtolnay/rust-toolchain@stable

    - name: Install cargo-audit
      run: cargo install cargo-audit

    - name: Run security audit
      run: cargo audit

    - name: Check for vulnerabilities
      run: |
        cargo install cargo-audit
        cargo audit --ignore-yaml --format json > audit-results.json || true

    - name: Upload security results
      uses: actions/upload-artifact@v4
      with:
        name: security-audit-results
        path: audit-results.json

  build:
    name: Build Release
    runs-on: ubuntu-latest
    strategy:
      matrix:
        target: [x86_64-unknown-linux-gnu, aarch64-unknown-linux-gnu]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y gcc-aarch64-linux-gnu

    - name: Install Rust
      uses: dtolnay/rust-toolchain@stable
      with:
        targets: ${{ matrix.target }}

    - name: Build release binary
      run: cargo build --release --target ${{ matrix.target }}
      env:
        CARGO_TARGET_AARCH64_UNKNOWN_LINUX_GNU_LINKER: aarch64-linux-gnu-gcc

    - name: Upload release artifacts
      uses: actions/upload-artifact@v4
      with:
        name: zthfs-${{ matrix.target }}
        path: target/${{ matrix.target }}/release/zthfs

  documentation:
    name: Documentation
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Install Rust
      uses: dtolnay/rust-toolchain@stable

    - name: Generate documentation
      run: cargo doc --no-deps --document-private-items

    - name: Check documentation links
      run: |
        cargo doc --no-deps
        # Add link checking here if needed

    - name: Upload documentation
      uses: actions/upload-artifact@v4
      with:
        name: documentation
        path: target/doc/

  integration:
    name: Integration Tests
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Install Rust
      uses: dtolnay/rust-toolchain@stable

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y fuse libfuse-dev

    - name: Run integration tests
      run: |
        # Note: Integration tests not yet implemented
        echo "Integration tests not yet implemented - skipping"
        # sudo -E PATH="$PATH" cargo test --test integration -- --nocapture
      env:
        USER: root
        HOME: /root

  performance:
    name: Performance Tests
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Install Rust
      uses: dtolnay/rust-toolchain@nightly
      with:
        components: llvm-tools-preview

    - name: Install performance tools
      run: |
        cargo install --git https://github.com/flamegraph-rs/flamegraph

    - name: Run performance benchmarks
      run: |
        echo "Running encryption benchmarks..."
        cargo bench --bench crypto_benchmarks -- --nocapture | tee crypto-benchmark-results.txt

        echo "Running integrity verification benchmarks..."
        cargo bench --bench integrity_benchmarks -- --nocapture | tee integrity-benchmark-results.txt

        echo "Running filesystem operations benchmarks..."
        cargo bench --bench filesystem_benchmarks -- --nocapture | tee filesystem-benchmark-results.txt

        echo "Combining all benchmark results..."
        echo "=== CRYPTO BENCHMARKS ===" > benchmark-results.txt
        cat crypto-benchmark-results.txt >> benchmark-results.txt
        echo -e "\n=== INTEGRITY BENCHMARKS ===" >> benchmark-results.txt
        cat integrity-benchmark-results.txt >> benchmark-results.txt
        echo -e "\n=== FILESYSTEM BENCHMARKS ===" >> benchmark-results.txt
        cat filesystem-benchmark-results.txt >> benchmark-results.txt

    - name: Install system performance tools
      run: |
        sudo apt-get update
        sudo apt-get install -y linux-tools-common linux-tools-generic
        echo "Performance tools installed successfully"

    - name: Generate flamegraph and perf data
      run: |
        # Enable debug symbols for flamegraph
        export CARGO_PROFILE_RELEASE_DEBUG=true

        echo "Checking perf availability and permissions..."
        PERF_AVAILABLE=false

        if command -v perf &> /dev/null; then
          echo "perf is available, checking permissions..."

          # Try to adjust perf_event_paranoid if possible
          if [ -w /proc/sys/kernel/perf_event_paranoid ]; then
            echo "Adjusting perf_event_paranoid setting..."
            sudo sysctl -w kernel.perf_event_paranoid=1 || echo "Failed to adjust perf_event_paranoid, trying with sudo perf..."
          fi

          # Test perf with a simple command
          if sudo perf stat --help > /dev/null 2>&1; then
            echo "perf with sudo works, using sudo for all perf commands..."
            PERF_AVAILABLE=true
            PERF_CMD="sudo perf"
          elif perf stat --help > /dev/null 2>&1; then
            echo "perf works without sudo..."
            PERF_AVAILABLE=true
            PERF_CMD="perf"
          else
            echo "perf not accessible, even with sudo..."
            PERF_AVAILABLE=false
          fi
        else
          echo "perf is not installed..."
          PERF_AVAILABLE=false
        fi

        if [ "$PERF_AVAILABLE" = "true" ]; then
          echo "Generating comprehensive performance analysis..."

          # Generate flamegraph for crypto operations
          echo "Generating flamegraph for crypto operations..."
          if ! cargo flamegraph --bin zthfs -- demo 2>/dev/null; then
            echo "Flamegraph generation failed, creating placeholder..."
            echo "Flamegraph generation failed - permission issues" > flamegraph.svg
          fi

          # Collect perf data for different benchmark suites with error handling
          echo "Collecting perf data for crypto benchmarks..."
          if ! $PERF_CMD record --call-graph=dwarf cargo bench --bench crypto_benchmarks -- --nocapture 2>/dev/null; then
            echo "Perf recording failed for crypto benchmarks"
          fi
          $PERF_CMD script > crypto-perf.script 2>/dev/null || echo "Failed to generate crypto perf script"
          $PERF_CMD report --stdio > crypto-perf-report.txt 2>/dev/null || echo "Failed to generate crypto perf report"

          echo "Collecting perf data for integrity benchmarks..."
          if ! $PERF_CMD record --call-graph=dwarf cargo bench --bench integrity_benchmarks -- --nocapture 2>/dev/null; then
            echo "Perf recording failed for integrity benchmarks"
          fi
          $PERF_CMD script > integrity-perf.script 2>/dev/null || echo "Failed to generate integrity perf script"
          $PERF_CMD report --stdio > integrity-perf-report.txt 2>/dev/null || echo "Failed to generate integrity perf report"

          echo "Collecting perf data for filesystem benchmarks..."
          if ! $PERF_CMD record --call-graph=dwarf cargo bench --bench filesystem_benchmarks -- --nocapture 2>/dev/null; then
            echo "Perf recording failed for filesystem benchmarks"
          fi
          $PERF_CMD script > filesystem-perf.script 2>/dev/null || echo "Failed to generate filesystem perf script"
          $PERF_CMD report --stdio > filesystem-perf-report.txt 2>/dev/null || echo "Failed to generate filesystem perf report"

          # Generate summary report
          echo "Performance analysis completed with some issues"

          # Create performance summary
          echo "=== PERFORMANCE TEST SUMMARY ===" > performance-summary.txt
          echo "Date: $(date)" >> performance-summary.txt
          echo "Commit: ${{ github.sha }}" >> performance-summary.txt
          echo "Branch: ${{ github.ref }}" >> performance-summary.txt
          echo "Performance analysis: Completed with perf (some components may have failed)" >> performance-summary.txt
          echo "" >> performance-summary.txt

          # Extract key metrics
          echo "=== KEY METRICS ===" >> performance-summary.txt
          echo "Crypto benchmarks completed" >> performance-summary.txt
          echo "Integrity benchmarks completed" >> performance-summary.txt
          echo "Filesystem benchmarks completed" >> performance-summary.txt
          echo "Flamegraph attempted" >> performance-summary.txt
          echo "" >> performance-summary.txt

          # Check for performance regressions (simple check)
          echo "=== PERFORMANCE REGRESSION CHECK ===" >> performance-summary.txt
          if grep -q "Performance has regressed" benchmark-results.txt; then
            echo "⚠️  Performance regression detected!" >> performance-summary.txt
            echo "Review benchmark results for details." >> performance-summary.txt
          else
            echo "✅ No performance regressions detected" >> performance-summary.txt
          fi

        else
          echo "perf is not available or accessible, running benchmarks without perf analysis"

          # Still run the benchmarks without perf
          echo "Running crypto benchmarks without perf..."
          cargo bench --bench crypto_benchmarks -- --nocapture | tee crypto-benchmark-results.txt || echo "Crypto benchmarks failed"

          echo "Running integrity benchmarks without perf..."
          cargo bench --bench integrity_benchmarks -- --nocapture | tee integrity-benchmark-results.txt || echo "Integrity benchmarks failed"

          echo "Running filesystem benchmarks without perf..."
          cargo bench --bench filesystem_benchmarks -- --nocapture | tee filesystem-benchmark-results.txt || echo "Filesystem benchmarks failed"

          echo "Creating placeholder files..."
          echo "Flamegraph generation skipped - perf not available" > flamegraph.svg
          echo "Perf analysis not available" > crypto-perf-report.txt
          echo "Perf analysis not available" > integrity-perf-report.txt
          echo "Perf analysis not available" > filesystem-perf-report.txt

          echo "=== PERFORMANCE TEST SUMMARY ===" > performance-summary.txt
          echo "Date: $(date)" >> performance-summary.txt
          echo "Commit: ${{ github.sha }}" >> performance-summary.txt
          echo "Branch: ${{ github.ref }}" >> performance-summary.txt
          echo "Performance analysis: Limited (perf not available or accessible)" >> performance-summary.txt
          echo "" >> performance-summary.txt

          echo "=== KEY METRICS ===" >> performance-summary.txt
          echo "Benchmarks run without perf analysis" >> performance-summary.txt
          echo "" >> performance-summary.txt

          echo "=== PERFORMANCE REGRESSION CHECK ===" >> performance-summary.txt
          if grep -q "Performance has regressed" benchmark-results.txt; then
            echo "⚠️  Performance regression detected!" >> performance-summary.txt
            echo "Review benchmark results for details." >> performance-summary.txt
          else
            echo "✅ No performance regressions detected" >> performance-summary.txt
          fi
        fi

    - name: Upload performance results
      uses: actions/upload-artifact@v4
      with:
        name: performance-results
        path: |
          benchmark-results.txt
          crypto-benchmark-results.txt
          integrity-benchmark-results.txt
          filesystem-benchmark-results.txt
          performance-summary.txt
          flamegraph.svg
          perf.data
          *.script
          *-perf-report.txt
